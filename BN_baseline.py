import argparse
import time
import pandas as pd
import numpy as np
import bnlearn as bn
from sklearn.mixture import BayesianGaussianMixture

def preprocess(df, n, name, v, max_iter):
    #This function discretize the entire dataset with VGMM
    data = df.copy()
    continuous = data.loc[:,[c for c in data.columns if c.split('_')[0] in ["In Byte", "Out Byte", "In Packet", "Out Packet", "Duration", "Time"]]]#The columns that need to be discretize
    dic = {}
    for col in continuous.columns:
        m = continuous[col].describe([.9])[5]#Ignore the outliers
        temp = continuous.loc[data[col]<=m, col].to_numpy().reshape(-1,1)#Train the VGMM only on the most representative data
        gm = BayesianGaussianMixture(n_components=n, random_state=0, verbose=v, max_iter = max_iter).fit(temp)
        dic[col] = {"edge":m, "max": continuous[col].max(), "weights": gm.weights_.squeeze(), "means": gm.means_.squeeze(), "std": gm.covariances_.squeeze()}#Get the argument of each gaussian kernel
        continuous.loc[data[col]<=m, col] = gm.predict(temp)#Transform the data, each data will get the index of its corresponding kernel
        continuous.loc[data[col]>m, col] = n#For all the oultliers, give the same value
        data[col] = continuous[col]
    return data.astype("category"), dic#return the discretized data and the parameter of all the kernels

def reconstruct(df, dic):
    #This function reconstrcut continuous data from the discretize data and the parameters of the kernels
    data = df.copy()
    continuous = data.loc[:,[c for c in  data.columns if c.split('_')[0] in ["In Byte", "Out Byte", "In Packet", "Out Packet", "Duration", "Time"]]].astype(int)#The continuous columns
    for col in continuous.columns:
        n = len(dic[col]["weights"])#number of kernels
        continuous.loc[continuous[col] == float(n), col] = np.random.uniform(dic[col]["edge"], dic[col]["max"], (data[col] == float(n)).sum())#Sample extreme value uniformly in the outlier range
        norm = continuous.loc[continuous[col] < float(n), col]
        mean = np.array(dic[col]["means"])[norm.values.astype(int)]
        std = np.array(dic[col]["std"])[norm.values.astype(int)]
        continuous.loc[continuous[col] < float(n), col] = np.random.normal(mean, np.sqrt(std))#Sample value based on their kernel
        continuous[col]=np.maximum(continuous[col], np.zeros(len(continuous)))#All numerical value should be positive
        if col != "Time":
            continuous[col]=continuous[col].round(1)#All numerical value except time should be integers
        data[col] = continuous[col]
    return data

def build_time_dep(df, steps=5):
    #This function transform the network flow dataset into a training dataset for SequenceBN
    res = pd.concat([df.shift(-i) for i in range(steps)], axis=1)#Each column is duplicated steps time, with a shift of 1 per dupplicated column
    res.columns = [f'{col}_{i+1}' for i in range(steps) for col in df.columns]#rename the columns
    for i in range(1, steps):#The time should be interarrival tim not absolute time
        res[f'Time_{i+1}'] = res[f'Date first seen_{i+1}']-res[f'Date first seen_{i}']
        res[f'Time_{i+1}'] = res[f'Time_{i+1}'].dt.total_seconds()*1e6 + res[f'Time_{i+1}'].dt.microseconds
    res = res.iloc[::steps,:]#We only take the line of index multiple of steps
    return res.dropna()
    
def regroup(df, steps):
    #This function tranform the data generated by SequenceBN back to the originalnetwork flows format
    initial_columns = set('_'.join(col.split('_')[:-1]) for col in df.columns)#Get the original columns
    res = []
    for initial_column in initial_columns:
        df_grouped = df[[c for c in df.columns if initial_column in c]]#Get all the columns relevant to reconstruct the original columns
        ser = []
        for col in sorted(df_grouped.columns):
            i = int(col.split('_')[-1])#Get the index
            s = df_grouped[col].copy()#Get the column
            if "Time" in col and i>1:#Particular case of the time
                s = s + ser[-1].reset_index(drop=True)
            s.name = initial_column
            s.index = s.index*steps + i-1#Redindex the columns
            ser.append(s)
        column = pd.concat(ser, axis=0).sort_index()#Recreate the ordered column
        res.append(column)
    reconstructed_df = pd.concat(res,axis=1)#Recreate the dataset
    reconstructed_df = reconstructed_df[['Proto','Src IP Addr','Dst IP Addr','Dst Pt','In Byte','Out Byte', 'In Packet', 'Out Packet', 'Flags', 'Duration', 'Day','Time']]#Reorder the dataset
    return reconstructed_df

if __name__ == '__main__':
    parser = argparse.ArgumentParser(prog="Bayesian Netwworks Baseline", description="Generate Netflow with bayesian networks")

    parser.add_argument("time_window", type=int, default=0)
    parser.add_argument("input_path", type=str, default = "data/train.csv")
    parser.add_argument("output_path", type=str, default = "data/")
    args = parser.parse_args()
    time_window = args.time_window
    independant_flow = time_window == 0
    path = args.input_path
    name = args.output_path+"BN_baseline_window"+str(time_window)+"_syn.csv"
    print(path)

    df = pd.read_csv(path)
    train = df.copy()
    
    print(len(df))

    t0 = time.time()
    
    #Preprocessing
    train["Date first seen"] = pd.to_datetime(train["Date first seen"])
    train["Day"] = train["Date first seen"].dt.dayofweek
    train["Time"] = train["Date first seen"].dt.time
    train["Time"] = pd.to_timedelta(train["Time"].astype(str))
    train["Time"] = train["Time"].dt.total_seconds()*1e6 + train["Time"].dt.microseconds
    
    #Build temporal dependencies if needed
    if not independant_flow:
        train = build_time_dep(train, time_window)
    #Preprocessing 2
    train_p, preprocess_dic = preprocess(train, 40, "standard_bn_stats.json", v=2, max_iter=50)

    train_p.drop([col for col in  train_p.columns if "Date first seen" in col], axis=1, inplace=True)

    train_p.dropna(inplace=True)
        
    t1= time.time()
    dif1 = divmod(int(t1-t0), 3600)
    print("Preprocessing time: {} hours, {} minutes, {} seconds".format(dif1[0], *divmod(dif1[1],60)))
    
    #Learn the structure of the network
    model = bn.structure_learning.fit(train_p, methodtype='hc', scoretype='bic')
    #Learn the parameters of the network
    model = bn.parameter_learning.fit(model, train_p, methodtype='bayes')
    
    t2= time.time()
    dif2 = divmod(int(t2-t1), 3600)
    print("Training time: {} hours, {} minutes, {} seconds".format(dif2[0], *divmod(dif2[1],60)))
    
    #Sample new data from the network
    new_data = bn.sampling(model,len(train_p))
    
    new_data = reconstruct(new_data,preprocess_dic)
    
    if not independant_flow:
    	new_data = regroup(new_data, time_window)
    
    new_data.dropna(inplace=True)

    #Reconstruct the date
    origTimestamp = pd.to_datetime(min(pd.to_datetime(df["Date first seen"]).dt.date))
    
    new_data.Day = new_data.Day.astype(int) - origTimestamp.dayofweek

    new_data["Date first seen"] = pd.to_timedelta(pd.to_timedelta(new_data.Time, unit='us').dt.total_seconds()+new_data.Day*24*3600,unit='s')+origTimestamp

    new_data.drop(["Day","Time"],axis=1,inplace=True)
    
    #Save the data
    new_data = new_data[df.columns]
    
    new_data.to_csv(name,index=False)
    
    t3= time.time()
    dif3 = divmod(int(t3-t2), 3600)
    print("Sampling time: {} hours, {} minutes, {} seconds".format(dif3[0], *divmod(dif3[1],60)))

